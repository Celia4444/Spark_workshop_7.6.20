{"cells":[{"cell_type":"markdown","source":["# Machine learning using Databricks\n\n### Logistic Regression model for Coronary Heart Disease Prediction"],"metadata":{}},{"cell_type":"markdown","source":["1. Data Cleaning in Spark\n2. Feature Engineering in Databricks\n3. Logistic regression model training & evaluation\n4. Model tuning"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["hd = spark.read.format(\"delta\").load(\"/newwave_workshop/predict_heart_disease/delta/heart_disease\")\nhd.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(hd)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Demographic:  \n• Sex: male or female(Nominal)  \n• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n\nBehavioral  \n• Current Smoker: whether or not the patient is a current smoker (Nominal)  \n• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.) \n\nMedical( history)  \n• BP Meds: whether or not the patient was on blood pressure medication (Nominal)  \n• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)  \n• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)  \n• Diabetes: whether or not the patient had diabetes (Nominal)  \n\nMedical(current)  \n• Tot Chol: total cholesterol level (Continuous)  \n• Sys BP: systolic blood pressure (Continuous)  \n• Dia BP: diastolic blood pressure (Continuous)  \n• BMI: Body Mass Index (Continuous)  \n• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)  \n• Glucose: glucose level (Continuous)  \n\nPredict variable (desired target)  \n• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)"],"metadata":{}},{"cell_type":"code","source":["hd.printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### 1. Data cleaning in Spark"],"metadata":{}},{"cell_type":"code","source":["display(hd.describe())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["hd = hd.withColumnRenamed('male','gender')"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(hd.select([count(when(col(c)=='NA',c)).alias(c) for c in hd.columns]))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# education (1,2,3,4 four levels)\ndisplay(hd.groupBy('education').count().orderBy('education'))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# currentSmoker(1,0)\n# cigsPerDay: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\ndisplay(hd.select('currentSmoker','cigsPerDay').where(col('cigsPerDay')=='NA'))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# BPMeds: whether or not the patient was on blood pressure medication (1:96%,0:3%)\ndisplay(hd.select('BPMeds'))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(hd.select('heartRate'))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# education: (1,2,3,4 four levels) use most populer one: 1\n# cigsPerDay:use average \n# BPMeds:whether or not the patient was on blood pressure medication (1:96%,0:3%), use most populer one: 0\n# totChol:total cholesterol level (Continuous),use average\n# BMI:Body Mass Index (Continuous),use average\n# heartRate:use average\n# glucose:glucose level,use average"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["avg_cigsPerDay = hd.where((col(\"currentSmoker\") == 1) & (col(\"cigsPerDay\") != 'NA')).agg(avg(\"cigsPerDay\")).first()[0]\navg_totChol = hd.where((col(\"totChol\") != 'NA')).agg(avg(\"totChol\")).first()[0]\navg_BMI = hd.where((col(\"BMI\") != 'NA')).agg(avg(\"BMI\")).first()[0]\navg_heartRate = hd.where((col(\"heartRate\") != 'NA')).agg(avg(\"heartRate\")).first()[0]\navg_glucose = hd.where((col(\"glucose\") != 'NA')).agg(avg(\"glucose\")).first()[0]"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# data imputation\n\nhd = (hd.withColumn(\"education_imp\", when(col(\"education\") == 'NA', 1).otherwise(col(\"education\")).cast(DoubleType()))\n        .withColumn(\"cigsPerDay_imp\", when(col(\"cigsPerDay\") == 'NA', avg_cigsPerDay).otherwise(col(\"cigsPerDay\")).cast(DoubleType()))\n        .withColumn(\"BPMeds_imp\", when(col(\"BPMeds\") == 'NA', 0).otherwise(col(\"BPMeds\")).cast(DoubleType()))\n        .withColumn(\"totChol_imp\", when(col(\"totChol\") == 'NA', avg_totChol).otherwise(col(\"totChol\")).cast(DoubleType()))\n        .withColumn(\"BMI_imp\", when(col(\"BMI\") == 'NA', avg_BMI).otherwise(col(\"BMI\")).cast(DoubleType()))\n        .withColumn(\"heartRate_imp\", when(col(\"heartRate\") == 'NA', avg_heartRate).otherwise(col(\"heartRate\")).cast(DoubleType()))\n        .withColumn(\"glucose_imp\", when(col(\"glucose\") == 'NA', avg_glucose).otherwise(col(\"glucose\")).cast(DoubleType()))\n     )"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(hd.describe())"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["####2. Feature Engineering in Databricks"],"metadata":{}},{"cell_type":"code","source":["display(hd.groupBy(\"TenYearCHD\").count().alias(\"count\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# BalancingRatio= numNegatives/dataset_size\nBalancingRatio = hd.where(col(\"TenYearCHD\") =='0').count()/hd.count()\nBalancingRatio"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# For Outcome = 1, we put BalancingRatio in column “classWeights”\n# For Outcome = 0, we put (1-BalancingRatio) in column “classWeights”\nhd1 = hd.withColumn(\"classWeights\", when(col(\"TenYearCHD\") =='1',BalancingRatio).otherwise(1-BalancingRatio))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["hd1.write.mode(\"overwrite\").format('delta').option(\"overwriteSchema\", True).save('/newwave_workshop/predict_heart_disease/delta/hd_for_lr/')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\n# We will use `VectorAssembler` to combine all feature columns into a feature vector (optimized data structure for ML)\ninput_cols = ['gender', 'age', 'currentSmoker', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'sysBP', 'diaBP', 'education_imp', 'cigsPerDay_imp', 'BPMeds_imp', 'totChol_imp', 'BMI_imp', 'heartRate_imp', 'glucose_imp']\n\nvectorAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\", handleInvalid='skip')\n\nhd2 = vectorAssembler.transform(hd1)\n\ndf = hd2.withColumnRenamed('TenYearCHD','label').select(\"features\", \"label\", \"classWeights\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(hd1)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### 3. Logistic regression model training & evaluation"],"metadata":{}},{"cell_type":"markdown","source":["https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression"],"metadata":{}},{"cell_type":"code","source":["#Logistic function\nk = 1.\nx0 = 0.\nx = np.arange(-10., 10, 0.1)\ny = [ 1/(1+ np.exp(-k * (x_ - x0))) for x_ in x]\ndisplay(plt.plot(x, y))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["15 features : ![15 features](http://www.sciweavers.org/upload/Tex2Img_1594121026/render.png)  \nLogistic function : ![Logistic Regression](http://www.sciweavers.org/upload/Tex2Img_1594120942/render.png)"],"metadata":{}},{"cell_type":"code","source":["train_df, test_df = df.randomSplit([0.75, 0.25])"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, weightCol=\"classWeights\" )\n\n# Train model with Training Data\nlrModel = lr.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["predictions = lrModel.transform(test_df)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# default metric for the BinaryClassificationEvaluator is areaUnderROC\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n\nprint(\"areaUnderROC is: \", evaluator.evaluate(predictions))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["https://en.wikipedia.org/wiki/Binary_classification\nhttps://en.wikipedia.org/wiki/Receiver_operating_characteristic#:~:text=A%20receiver%20operating%20characteristic%20curve,its%20discrimination%20threshold%20is%20varied."],"metadata":{}},{"cell_type":"code","source":["df_result = predictions.select(\"label\", \"prediction\").toPandas()\n\ncnf_matrix = pd.DataFrame(df_result.groupby(['label','prediction']).size()).reset_index()\ncnf_matrix.columns=['label', 'prediction', 'count']\ncnf_matrix"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["sensitivity = predictions.where(col('label')==1).agg(avg('prediction')).first()[0]\nspecificity = predictions.where(col('label')==0).agg(avg(lit(1) - col('prediction'))).first()[0]\nprint(\"sensitivity is: \", sensitivity)\nprint(\"specificity is: \", specificity)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["####4. Model tuning"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["paramGrid"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Cross Validation\n\nhttps://en.wikipedia.org/wiki/Cross-validation_(statistics)#:~:text=Cross%2Dvalidation%2C%20sometimes%20called%20rotation,to%20an%20independent%20data%20set."],"metadata":{}},{"cell_type":"code","source":["lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", weightCol=\"classWeights\")\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(train_df)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["cvModel.bestModel.intercept"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["cvModel.bestModel.coefficientMatrix"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Use test set to measure the accuracy of our model on new data\npredictions = cvModel.transform(test_df)\n\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nprint(\"areaUnderROC is: \", evaluator.evaluate(predictions))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["df_result = predictions.select(\"label\", \"prediction\").toPandas()\n\ncnf_matrix = pd.DataFrame(df_result.groupby(['label','prediction']).size()).reset_index()\ncnf_matrix.columns=['label', 'prediction', 'count']\ncnf_matrix"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["sensitivity = predictions.where(col('label')==1).agg(avg('prediction')).first()[0]\nspecificity = predictions.where(col('label')==0).agg(avg(lit(1) - col('prediction'))).first()[0]\nprint(\"sensitivity is: \", sensitivity)\nprint(\"specificity is: \", specificity)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["&copy; 2020 NewWave. Data Science Group"],"metadata":{}}],"metadata":{"name":"2 - Logistic regression","notebookId":644406898324548},"nbformat":4,"nbformat_minor":0}
